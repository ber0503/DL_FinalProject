{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49303fb47d1348e59e70e23a8910fd6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a292d11390b424faa8fd165a470f15e",
              "IPY_MODEL_9db12e2e64f04debaf5e8b02cffe59ff",
              "IPY_MODEL_6912bc9ff8534790bca9820703b2ca77"
            ],
            "layout": "IPY_MODEL_3080e6f1b06f4b0c8c570631a7ae1f0b"
          }
        },
        "2a292d11390b424faa8fd165a470f15e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f9dcee6ae174fdfb27aee775280df57",
            "placeholder": "​",
            "style": "IPY_MODEL_23f06df90f134992bce7da589f145434",
            "value": "model.safetensors: 100%"
          }
        },
        "9db12e2e64f04debaf5e8b02cffe59ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f77e906ad994a54bd156984247fe39b",
            "max": 88216496,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ae742bef46b4a6689042ddb2fc1d14a",
            "value": 88216496
          }
        },
        "6912bc9ff8534790bca9820703b2ca77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9243c209fae849dabf4ead2dea098344",
            "placeholder": "​",
            "style": "IPY_MODEL_0a6ed8ecc73348b6a63c6455b8e7b5e1",
            "value": " 88.2M/88.2M [00:02&lt;00:00, 73.5MB/s]"
          }
        },
        "3080e6f1b06f4b0c8c570631a7ae1f0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f9dcee6ae174fdfb27aee775280df57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23f06df90f134992bce7da589f145434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f77e906ad994a54bd156984247fe39b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ae742bef46b4a6689042ddb2fc1d14a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9243c209fae849dabf4ead2dea098344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a6ed8ecc73348b6a63c6455b8e7b5e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1eGMxpIWy9D"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import timm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model"
      ],
      "metadata": {
        "id": "ozld0J87Xw1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"ViT fine-tuning on CIFAR-100 with time/epoch constraints\"\n",
        "    )\n",
        "    parser.add_argument(\"--data_root\", type=str, default=\"./data\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "    parser.add_argument(\"--num_workers\", type=int, default=4)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5)\n",
        "    parser.add_argument(\"--lr\", type=float, default=5e-5)\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--model_name\", type=str,\n",
        "                        default=\"vit_small_patch16_224\")\n",
        "    parser.add_argument(\"--out_dir\", type=str,\n",
        "                        default=\"./ckpt_vit_cifar100\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    parser.add_argument(\"--use_amp\", action=\"store_true\")\n",
        "    parser.add_argument(\"--subset_ratio\", type=float, default=1.0)\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n"
      ],
      "metadata": {
        "id": "kmNW6cuuXOU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n"
      ],
      "metadata": {
        "id": "RfxfqokQXUff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloaders(data_root, batch_size, num_workers,\n",
        "                    subset_ratio=1.0, seed=42):\n",
        "    # CIFAR-100: 32x32 -> resize 224x224 for ViT\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    train_set = datasets.CIFAR100(\n",
        "        root=data_root, train=True, download=True, transform=train_transform\n",
        "    )\n",
        "    test_set = datasets.CIFAR100(\n",
        "        root=data_root, train=False, download=True, transform=test_transform\n",
        "    )\n",
        "\n",
        "    if subset_ratio < 1.0:\n",
        "        assert subset_ratio > 0.0\n",
        "        np.random.seed(seed)\n",
        "        indices = np.random.permutation(len(train_set))\n",
        "        k = int(len(train_set) * subset_ratio)\n",
        "        indices = indices[:k]\n",
        "        train_set = Subset(train_set, indices)\n",
        "        print(f\"Using subset of train set: {k} samples \"\n",
        "              f\"({subset_ratio*100:.1f}% of 50k)\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_set, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_set, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "    return train_loader, test_loader\n"
      ],
      "metadata": {
        "id": "9SujfwijXYOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(model_name: str, num_classes: int = 100):\n",
        "    # 1. 创建 ViT 预训练模型\n",
        "    model = timm.create_model(\n",
        "        model_name,\n",
        "        pretrained=True,\n",
        "        num_classes=num_classes\n",
        "    )\n",
        "\n",
        "    # 2. LoRA 配置\n",
        "    # lora_config = LoraConfig(\n",
        "    #     r=8,                       # LoRA rank（8 在 Colab 很稳）\n",
        "    #     lora_alpha=16,\n",
        "    #     lora_dropout=0.1,\n",
        "    #     bias=\"none\",\n",
        "    #     target_modules=[\n",
        "    #         \"qkv\",                 # ViT attention 里的 QKV\n",
        "    #         \"proj\"                 # attention 输出 projection\n",
        "    #     ],\n",
        "    # )\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,                     # 比 8 稍大\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\n",
        "             \"qkv\",\n",
        "             \"proj\",\n",
        "             \"fc1\",                # MLP\n",
        "             \"fc2\",\n",
        "         ],\n",
        "    )\n",
        "\n",
        "\n",
        "    # 3. 注入 LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # 4. 打印可训练参数比例（强烈建议保留）\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "I295UwFTXbnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer,\n",
        "                    device, epoch, scaler=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start = time.time()\n",
        "\n",
        "    for i, (images, targets) in enumerate(loader):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += preds.eq(targets).sum().item()\n",
        "\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"Epoch [{epoch}] Step [{i+1}/{len(loader)}] \"\n",
        "                  f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    acc = correct / total * 100.0\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"Train Epoch {epoch}: Loss {epoch_loss:.4f}, \"\n",
        "          f\"Acc {acc:.2f}%, Time {elapsed/60:.2f} min\")\n",
        "    return epoch_loss, acc\n",
        "\n"
      ],
      "metadata": {
        "id": "DhkxoxuZXgoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += preds.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    acc = correct / total * 100.0\n",
        "    print(f\"Val : Loss {epoch_loss:.4f}, Acc {acc:.2f}%\")\n",
        "    return epoch_loss, acc"
      ],
      "metadata": {
        "id": "SXDj02a6XkYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    args = parse_args()\n",
        "    assert args.epochs <= 5, \"Requirement: epochs must be <= 5\"\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    out_dir = Path(args.out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_loader, val_loader = get_dataloaders(\n",
        "        args.data_root,\n",
        "        args.batch_size,\n",
        "        args.num_workers,\n",
        "        subset_ratio=args.subset_ratio,\n",
        "        seed=args.seed,\n",
        "    )\n",
        "\n",
        "    model = create_model(args.model_name, num_classes=100)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(\n",
        "      filter(lambda p: p.requires_grad, model.parameters()),\n",
        "      lr=args.lr,\n",
        "      weight_decay=args.weight_decay\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=args.epochs\n",
        "    )\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler() if args.use_amp and device == \"cuda\" else None\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, device, epoch, scaler\n",
        "        )\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            ckpt_path = out_dir / \"best_vit_finetune.pth\"\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model_state\": model.state_dict(),\n",
        "                    \"optimizer_state\": optimizer.state_dict(),\n",
        "                    \"best_acc\": best_acc,\n",
        "                },\n",
        "                ckpt_path,\n",
        "            )\n",
        "            print(f\"Saved best model to {ckpt_path} \"\n",
        "                  f\"(acc={best_acc:.2f}%)\")\n",
        "\n",
        "    print(f\"Training finished. Best val acc: {best_acc:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "49303fb47d1348e59e70e23a8910fd6f",
            "2a292d11390b424faa8fd165a470f15e",
            "9db12e2e64f04debaf5e8b02cffe59ff",
            "6912bc9ff8534790bca9820703b2ca77",
            "3080e6f1b06f4b0c8c570631a7ae1f0b",
            "2f9dcee6ae174fdfb27aee775280df57",
            "23f06df90f134992bce7da589f145434",
            "9f77e906ad994a54bd156984247fe39b",
            "1ae742bef46b4a6689042ddb2fc1d14a",
            "9243c209fae849dabf4ead2dea098344",
            "0a6ed8ecc73348b6a63c6455b8e7b5e1"
          ]
        },
        "id": "rK1IXhYTXoGQ",
        "outputId": "fed63ec6-bc51-4e4a-b9a9-11d1e6c8b492"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:05<00:00, 29.3MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49303fb47d1348e59e70e23a8910fd6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/88.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 599,040 || all params: 22,303,204 || trainable%: 2.6859\n",
            "Epoch [1] Step [50/1563] Loss: 4.8672\n",
            "Epoch [1] Step [100/1563] Loss: 4.7109\n",
            "Epoch [1] Step [150/1563] Loss: 4.8424\n",
            "Epoch [1] Step [200/1563] Loss: 4.6571\n",
            "Epoch [1] Step [250/1563] Loss: 4.3957\n",
            "Epoch [1] Step [300/1563] Loss: 4.4307\n",
            "Epoch [1] Step [350/1563] Loss: 3.9899\n",
            "Epoch [1] Step [400/1563] Loss: 3.7926\n",
            "Epoch [1] Step [450/1563] Loss: 3.4201\n",
            "Epoch [1] Step [500/1563] Loss: 3.4343\n",
            "Epoch [1] Step [550/1563] Loss: 3.1962\n",
            "Epoch [1] Step [600/1563] Loss: 2.8784\n",
            "Epoch [1] Step [650/1563] Loss: 2.6581\n",
            "Epoch [1] Step [700/1563] Loss: 2.2470\n",
            "Epoch [1] Step [750/1563] Loss: 2.4545\n",
            "Epoch [1] Step [800/1563] Loss: 2.4275\n",
            "Epoch [1] Step [850/1563] Loss: 2.5353\n",
            "Epoch [1] Step [900/1563] Loss: 2.1814\n",
            "Epoch [1] Step [950/1563] Loss: 1.9161\n",
            "Epoch [1] Step [1000/1563] Loss: 2.0632\n",
            "Epoch [1] Step [1050/1563] Loss: 2.3556\n",
            "Epoch [1] Step [1100/1563] Loss: 2.0974\n",
            "Epoch [1] Step [1150/1563] Loss: 2.4786\n",
            "Epoch [1] Step [1200/1563] Loss: 1.4199\n",
            "Epoch [1] Step [1250/1563] Loss: 1.8792\n",
            "Epoch [1] Step [1300/1563] Loss: 2.1307\n",
            "Epoch [1] Step [1350/1563] Loss: 1.9547\n",
            "Epoch [1] Step [1400/1563] Loss: 1.7274\n",
            "Epoch [1] Step [1450/1563] Loss: 2.0177\n",
            "Epoch [1] Step [1500/1563] Loss: 2.0357\n",
            "Epoch [1] Step [1550/1563] Loss: 1.7673\n",
            "Train Epoch 1: Loss 2.8920, Acc 34.61%, Time 8.65 min\n",
            "Val : Loss 1.3250, Acc 69.38%\n",
            "Saved best model to ckpt_vit_cifar100/best_vit_finetune.pth (acc=69.38%)\n",
            "Epoch [2] Step [50/1563] Loss: 2.0569\n",
            "Epoch [2] Step [100/1563] Loss: 1.7189\n",
            "Epoch [2] Step [150/1563] Loss: 1.4416\n",
            "Epoch [2] Step [200/1563] Loss: 1.5131\n",
            "Epoch [2] Step [250/1563] Loss: 1.1182\n",
            "Epoch [2] Step [300/1563] Loss: 1.7951\n",
            "Epoch [2] Step [350/1563] Loss: 1.7825\n",
            "Epoch [2] Step [400/1563] Loss: 1.2848\n",
            "Epoch [2] Step [450/1563] Loss: 1.5828\n",
            "Epoch [2] Step [500/1563] Loss: 1.4756\n",
            "Epoch [2] Step [550/1563] Loss: 1.4681\n",
            "Epoch [2] Step [600/1563] Loss: 1.5380\n",
            "Epoch [2] Step [650/1563] Loss: 2.0573\n",
            "Epoch [2] Step [700/1563] Loss: 1.3166\n",
            "Epoch [2] Step [750/1563] Loss: 1.2878\n",
            "Epoch [2] Step [800/1563] Loss: 1.3541\n",
            "Epoch [2] Step [850/1563] Loss: 1.5837\n",
            "Epoch [2] Step [900/1563] Loss: 1.4834\n",
            "Epoch [2] Step [950/1563] Loss: 1.1363\n",
            "Epoch [2] Step [1000/1563] Loss: 1.0975\n",
            "Epoch [2] Step [1050/1563] Loss: 1.5018\n",
            "Epoch [2] Step [1100/1563] Loss: 1.5794\n",
            "Epoch [2] Step [1150/1563] Loss: 1.8401\n",
            "Epoch [2] Step [1200/1563] Loss: 0.9164\n",
            "Epoch [2] Step [1250/1563] Loss: 1.5858\n",
            "Epoch [2] Step [1300/1563] Loss: 1.4392\n",
            "Epoch [2] Step [1350/1563] Loss: 1.4294\n",
            "Epoch [2] Step [1400/1563] Loss: 1.1317\n",
            "Epoch [2] Step [1450/1563] Loss: 1.0540\n",
            "Epoch [2] Step [1500/1563] Loss: 1.4124\n",
            "Epoch [2] Step [1550/1563] Loss: 1.5028\n",
            "Train Epoch 2: Loss 1.4269, Acc 65.29%, Time 8.65 min\n",
            "Val : Loss 0.8592, Acc 79.71%\n",
            "Saved best model to ckpt_vit_cifar100/best_vit_finetune.pth (acc=79.71%)\n",
            "Epoch [3] Step [50/1563] Loss: 1.4983\n",
            "Epoch [3] Step [100/1563] Loss: 1.1182\n",
            "Epoch [3] Step [150/1563] Loss: 1.0832\n",
            "Epoch [3] Step [200/1563] Loss: 0.8149\n",
            "Epoch [3] Step [250/1563] Loss: 1.5426\n",
            "Epoch [3] Step [300/1563] Loss: 1.0592\n",
            "Epoch [3] Step [350/1563] Loss: 1.3125\n",
            "Epoch [3] Step [400/1563] Loss: 1.0168\n",
            "Epoch [3] Step [450/1563] Loss: 1.3015\n",
            "Epoch [3] Step [500/1563] Loss: 0.7973\n",
            "Epoch [3] Step [550/1563] Loss: 1.2086\n",
            "Epoch [3] Step [600/1563] Loss: 0.9957\n",
            "Epoch [3] Step [650/1563] Loss: 1.4332\n",
            "Epoch [3] Step [700/1563] Loss: 1.2764\n",
            "Epoch [3] Step [750/1563] Loss: 0.9613\n",
            "Epoch [3] Step [800/1563] Loss: 1.4209\n",
            "Epoch [3] Step [850/1563] Loss: 0.9639\n",
            "Epoch [3] Step [900/1563] Loss: 1.1095\n",
            "Epoch [3] Step [950/1563] Loss: 1.4773\n",
            "Epoch [3] Step [1000/1563] Loss: 0.7183\n",
            "Epoch [3] Step [1050/1563] Loss: 1.6040\n",
            "Epoch [3] Step [1100/1563] Loss: 1.2852\n",
            "Epoch [3] Step [1150/1563] Loss: 1.1383\n",
            "Epoch [3] Step [1200/1563] Loss: 1.1497\n",
            "Epoch [3] Step [1250/1563] Loss: 0.8547\n",
            "Epoch [3] Step [1300/1563] Loss: 1.1922\n",
            "Epoch [3] Step [1350/1563] Loss: 1.2754\n",
            "Epoch [3] Step [1400/1563] Loss: 1.0996\n",
            "Epoch [3] Step [1450/1563] Loss: 1.3295\n",
            "Epoch [3] Step [1500/1563] Loss: 1.6187\n",
            "Epoch [3] Step [1550/1563] Loss: 1.5086\n",
            "Train Epoch 3: Loss 1.1687, Acc 71.08%, Time 8.66 min\n",
            "Val : Loss 0.7115, Acc 82.30%\n",
            "Saved best model to ckpt_vit_cifar100/best_vit_finetune.pth (acc=82.30%)\n",
            "Epoch [4] Step [50/1563] Loss: 1.2663\n",
            "Epoch [4] Step [100/1563] Loss: 0.8007\n",
            "Epoch [4] Step [150/1563] Loss: 1.3431\n",
            "Epoch [4] Step [200/1563] Loss: 1.0475\n",
            "Epoch [4] Step [250/1563] Loss: 1.4174\n",
            "Epoch [4] Step [300/1563] Loss: 1.1074\n",
            "Epoch [4] Step [350/1563] Loss: 0.8412\n",
            "Epoch [4] Step [400/1563] Loss: 0.7726\n",
            "Epoch [4] Step [450/1563] Loss: 1.2086\n",
            "Epoch [4] Step [500/1563] Loss: 1.1095\n",
            "Epoch [4] Step [550/1563] Loss: 0.9186\n",
            "Epoch [4] Step [600/1563] Loss: 0.6551\n",
            "Epoch [4] Step [650/1563] Loss: 0.8191\n",
            "Epoch [4] Step [700/1563] Loss: 0.9671\n",
            "Epoch [4] Step [750/1563] Loss: 1.2367\n",
            "Epoch [4] Step [800/1563] Loss: 0.7265\n",
            "Epoch [4] Step [850/1563] Loss: 1.2083\n",
            "Epoch [4] Step [900/1563] Loss: 1.0026\n",
            "Epoch [4] Step [950/1563] Loss: 0.6037\n",
            "Epoch [4] Step [1000/1563] Loss: 1.0855\n",
            "Epoch [4] Step [1050/1563] Loss: 0.6947\n",
            "Epoch [4] Step [1100/1563] Loss: 0.9900\n",
            "Epoch [4] Step [1150/1563] Loss: 0.5961\n",
            "Epoch [4] Step [1200/1563] Loss: 0.9333\n",
            "Epoch [4] Step [1250/1563] Loss: 0.8408\n",
            "Epoch [4] Step [1300/1563] Loss: 1.0021\n",
            "Epoch [4] Step [1350/1563] Loss: 1.4898\n",
            "Epoch [4] Step [1400/1563] Loss: 1.2208\n",
            "Epoch [4] Step [1450/1563] Loss: 1.1103\n",
            "Epoch [4] Step [1500/1563] Loss: 1.0989\n",
            "Epoch [4] Step [1550/1563] Loss: 1.1631\n",
            "Train Epoch 4: Loss 1.0494, Acc 73.58%, Time 8.67 min\n",
            "Val : Loss 0.6502, Acc 83.94%\n",
            "Saved best model to ckpt_vit_cifar100/best_vit_finetune.pth (acc=83.94%)\n",
            "Epoch [5] Step [50/1563] Loss: 1.4053\n",
            "Epoch [5] Step [100/1563] Loss: 0.7510\n",
            "Epoch [5] Step [150/1563] Loss: 0.9249\n",
            "Epoch [5] Step [200/1563] Loss: 0.7789\n",
            "Epoch [5] Step [250/1563] Loss: 0.7386\n",
            "Epoch [5] Step [300/1563] Loss: 1.3632\n",
            "Epoch [5] Step [350/1563] Loss: 1.0319\n",
            "Epoch [5] Step [400/1563] Loss: 0.7537\n",
            "Epoch [5] Step [450/1563] Loss: 0.9377\n",
            "Epoch [5] Step [500/1563] Loss: 0.9960\n",
            "Epoch [5] Step [550/1563] Loss: 1.0985\n",
            "Epoch [5] Step [600/1563] Loss: 0.6388\n",
            "Epoch [5] Step [650/1563] Loss: 0.9832\n",
            "Epoch [5] Step [700/1563] Loss: 0.6912\n",
            "Epoch [5] Step [750/1563] Loss: 1.3323\n",
            "Epoch [5] Step [800/1563] Loss: 0.6232\n",
            "Epoch [5] Step [850/1563] Loss: 1.0552\n",
            "Epoch [5] Step [900/1563] Loss: 0.9818\n",
            "Epoch [5] Step [950/1563] Loss: 0.8214\n",
            "Epoch [5] Step [1000/1563] Loss: 1.0053\n",
            "Epoch [5] Step [1050/1563] Loss: 0.8306\n",
            "Epoch [5] Step [1100/1563] Loss: 0.9857\n",
            "Epoch [5] Step [1150/1563] Loss: 1.0761\n",
            "Epoch [5] Step [1200/1563] Loss: 0.6910\n",
            "Epoch [5] Step [1250/1563] Loss: 1.0595\n",
            "Epoch [5] Step [1300/1563] Loss: 1.3399\n",
            "Epoch [5] Step [1350/1563] Loss: 0.7757\n",
            "Epoch [5] Step [1400/1563] Loss: 0.6212\n",
            "Epoch [5] Step [1450/1563] Loss: 0.9122\n",
            "Epoch [5] Step [1500/1563] Loss: 0.7692\n",
            "Epoch [5] Step [1550/1563] Loss: 0.9331\n",
            "Train Epoch 5: Loss 1.0064, Acc 74.70%, Time 8.66 min\n",
            "Val : Loss 0.6290, Acc 84.40%\n",
            "Saved best model to ckpt_vit_cifar100/best_vit_finetune.pth (acc=84.40%)\n",
            "Training finished. Best val acc: 84.40%\n"
          ]
        }
      ]
    }
  ]
}